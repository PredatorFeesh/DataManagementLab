{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "file = 'complaints_small.csv'\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master('') \\\n",
    "    .appName('') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Convert list to data frame\n",
    "df = spark.read.format('csv') \\\n",
    "                .option('header',True) \\\n",
    "                .option('multiLine', True) \\\n",
    "                .option(\"escape\", \"\\\"\") \\\n",
    "                .csv(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_complaints(x) :\n",
    "    prodDate, comp = x[0], x[1]\n",
    "    companies = list(comp)\n",
    "    ucompanies = list(set(companies))\n",
    "    # By taking the total number of companies in this list, we can get our total number of complaints.\n",
    "    numComplaints = len(companies)\n",
    "    # By taking the number of unique companies, we get the number of compaines complaints were against.\n",
    "    uNumCompanies = len(ucompanies)\n",
    "    # yields: (product, year), (number of complaints, number of companies)\n",
    "    yield (prodDate, (companies, numComplaints, uNumCompanies) )\n",
    "\n",
    "def map_pyc(x):\n",
    "    prodDate, compData = x[0], x[1]\n",
    "    # We want to combine product year company into one key\n",
    "    # This does introduce redundent data, but also allows us to get the total number of\n",
    "    # unique complaints per company.\n",
    "\n",
    "    # The plan here is to use the combiner in order to get the total number of complaints for company/prod/year\n",
    "    # thus grouping this redundant data over N times, given we have N compltains against a \n",
    "    # given (company, product, year).\n",
    "    listCompData = list(compData)\n",
    "    for compName in listCompData[0]: # For each company\n",
    "        # yields: (product, year, company), (number of compltaints, number of companies)\n",
    "        yield ((prodDate[0], prodDate[1], compName), (listCompData[1:3])) \n",
    "\n",
    "        \n",
    "def reduce_percentageComplaints(x):\n",
    "    pyc, compData = x[0], x[1]\n",
    "    # Now we want to find how many pieces of data we atually get\n",
    "    # This will because of the combine step between map and reduce, get  us the number\n",
    "    # of complaints against the given company in the year and product. We can then divide\n",
    "    # this by the total complaints against this year, product combo in the data.\n",
    "    pyc, compData = list(pyc), list(compData)\n",
    "    numComp = len(compData)\n",
    "    percentage = numComp / compData[0][0] # All compData is the same, so we can look at the very first one\n",
    "    # (product, year), (percent complaint against company, total prod/year complaints, total #companies complained prod/year)\n",
    "    yield ((pyc[0], pyc[1]), (percentage, compData[0][0], compData[0][1]))\n",
    "\n",
    "def reduce_maxComplaints(x):\n",
    "    py, compData = x[0], x[1]\n",
    "    # Now we want to extract out all the percentages so we can max.\n",
    "    # Note that totalRep and totalComp are all the same.\n",
    "    percent, totalRep, totalComp = zip(*list(compData))\n",
    "    roun = round(max(percent) * 100)\n",
    "\n",
    "    # Because totalRep and totalComp are all the same value, we just take the first.\n",
    "    yield ((py[0]), (int(py[1]), totalRep[0], totalComp[0], roun))\n",
    "\n",
    "def mapSequence(x):\n",
    "    k, v = x[0], x[1]\n",
    "    yield (k, v[0], v[1], v[2], v[3])\n",
    "    \n",
    "    \n",
    "rdd = df.rdd.map(tuple)\n",
    "# yields: (product, year), (company)\n",
    "rdd = rdd \\\n",
    ".filter(lambda x: x is not None and x[13] is not None and x[7] is not None and x[1] is not None) \\\n",
    ".map(lambda x: ((x[1], x[13][0:4]), x[7])).groupByKey() \\\n",
    ".flatMap(reduce_complaints) \\\n",
    ".flatMap(map_pyc).groupByKey() \\\n",
    ".flatMap(reduce_percentageComplaints).groupByKey() \\\n",
    ".flatMap(reduce_maxComplaints) \\\n",
    ".sortByKey() \\\n",
    ".flatMap(mapSequence) \\\n",
    ".saveAsTextFile('output')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "\n",
    "\n",
    "def reduce_complaints(x) :\n",
    "    prodDate, comp = x[0], x[1]\n",
    "    companies = list(comp)\n",
    "    ucompanies = list(set(companies))\n",
    "    # By taking the total number of companies in this list, we can get our total number of complaints.\n",
    "    numComplaints = len(companies)\n",
    "    # By taking the number of unique companies, we get the number of compaines complaints were against.\n",
    "    uNumCompanies = len(ucompanies)\n",
    "    # yields: (product, year), (number of complaints, number of companies)\n",
    "    yield (prodDate, (companies, numComplaints, uNumCompanies) )\n",
    "\n",
    "def map_pyc(x):\n",
    "    prodDate, compData = x[0], x[1]\n",
    "    # We want to combine product year company into one key\n",
    "    # This does introduce redundent data, but also allows us to get the total number of\n",
    "    # unique complaints per company.\n",
    "\n",
    "    # The plan here is to use the combiner in order to get the total number of complaints for company/prod/year\n",
    "    # thus grouping this redundant data over N times, given we have N compltains against a \n",
    "    # given (company, product, year).\n",
    "    listCompData = list(compData)\n",
    "    for compName in listCompData[0]: # For each company\n",
    "        # yields: (product, year, company), (number of compltaints, number of companies)\n",
    "        yield ((prodDate[0], prodDate[1], compName), (listCompData[1:3])) \n",
    "\n",
    "        \n",
    "def reduce_percentageComplaints(x):\n",
    "    pyc, compData = x[0], x[1]\n",
    "    # Now we want to find how many pieces of data we atually get\n",
    "    # This will because of the combine step between map and reduce, get  us the number\n",
    "    # of complaints against the given company in the year and product. We can then divide\n",
    "    # this by the total complaints against this year, product combo in the data.\n",
    "    pyc, compData = list(pyc), list(compData)\n",
    "    numComp = len(compData)\n",
    "    percentage = float(numComp) / float(compData[0][0]) # All compData is the same, so we can look at the very first one\n",
    "    # (product, year), (percent complaint against company, total prod/year complaints, total #companies complained prod/year)\n",
    "    yield ((pyc[0], pyc[1]), (percentage, compData[0][0], compData[0][1]))\n",
    "\n",
    "def reduce_maxComplaints(x):\n",
    "    py, compData = x[0], x[1]\n",
    "    # Now we want to extract out all the percentages so we can max.\n",
    "    # Note that totalRep and totalComp are all the same.\n",
    "    percent, totalRep, totalComp = zip(*list(compData))\n",
    "    roun = round(max(percent) * 100)\n",
    "\n",
    "    # Because totalRep and totalComp are all the same value, we just take the first.\n",
    "    yield py[0], py[1], totalRep[0], totalComp[0], roun\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    file =  'complaints_small.csv'\n",
    "    output = 'output'\n",
    "\n",
    "\n",
    "# Create Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .master('') \\\n",
    "        .appName('') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Convert list to data frame\n",
    "    df = spark.read.format('csv') \\\n",
    "                    .option('header',True) \\\n",
    "                    .option('multiLine', True) \\\n",
    "                    .option(\"escape\", \"\\\"\") \\\n",
    "                    .csv(file)\n",
    "\n",
    "    rdd = df.rdd.map(tuple)\n",
    "# yields: (product, year), (company)\n",
    "    rdd = rdd \\\n",
    "    .filter(lambda x: x is not None and x[13] is not None and x[7] is not None and x[1] is not None) \\\n",
    "    .map(lambda x: ((x[1], x[13][0:4]), x[7])).groupByKey() \\\n",
    "    .flatMap(reduce_complaints) \\\n",
    "    .flatMap(map_pyc).groupByKey() \\\n",
    "    .flatMap(reduce_percentageComplaints).groupByKey() \\\n",
    "    .flatMap(reduce_maxComplaints) \\\n",
    "    .saveAsTextFile(output)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
