{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.157:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc.stop()\n",
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're creating two RDD, one is from the book.txt file and the\n",
    "# other is directly from a list within the notebook.\n",
    "\n",
    "rdd = sc.textFile('book.txt')\n",
    "testRdd = sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Project', 'Gutenberg', 'EBook', 'of', 'English', 'Coins', 'and', 'Tokens,', 'by'], ['Llewellynn', 'Jewitt', 'and', 'Barclay', 'V.', 'Head'], []]\n",
      "\n",
      "['The', 'Project', 'Gutenberg']\n"
     ]
    }
   ],
   "source": [
    "# Here, we're showing the difference between map() and flatMap()\n",
    "# doing the line split and get back the first 3 elements, take(3).\n",
    "# - map() is a one-to-one mapping, just like Python, so the first\n",
    "# line prints out 3 lists, each consists words per each.\n",
    "# - flatMap() is a one-to-many mapping, like MapReduce's map(). So\n",
    "# the second line prints out only 3 words.\n",
    "\n",
    "wordsPerLine = rdd.map(lambda line: line.split()).take(3)\n",
    "words = rdd.flatMap(lambda line: line.split()).take(3)\n",
    "\n",
    "print ('%s\\n\\n%s' % (wordsPerLine, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 83), ('gutenberg', 25)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the word count example with Spark using the approach\n",
    "# shown in the slides, i.e. staying true to the MapReduce paradigm.\n",
    "# Note that groupByKey() will sort and group everything together by\n",
    "# keys first. Then the function in mapValues() will each get applied\n",
    "# per each (key, list of values) pair. This could be an issue if we\n",
    "# have a pair with lots of values since all of the values have to be\n",
    "# stored in memory.\n",
    "\n",
    "wc = rdd.flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda x: (x.lower(), 1)) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda values: sum(values))\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 83), ('gutenberg', 25)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is another approach with reduceByKey() instead of groupByKey().\n",
    "# The reduce function provided for reduceByKey() only takes 2 params\n",
    "# at a time, thus, doesn't suffer the scalability issue. It also has\n",
    "# better benefits in term of parallelism.\n",
    "\n",
    "wc = rdd.flatMap(lambda line: line.split()) \\\n",
    "        .map(lambda x: (x.lower(), 1)) \\\n",
    "        .reduceByKey(lambda x,y: x+y)\n",
    "wc.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2428), ('of', 1704), ('and', 1304)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we'd like to compute the top 3 most popular words in Spark. We\n",
    "# can use the RDD's top() function directly. This is much easier\n",
    "# than the two-step MapReduce job, where we had to first compute the\n",
    "# top 3 words per partition, then another top 3 on top of that. In\n",
    "# fact, this is exactly how Spark RDD's top() function is implemented.\n",
    "# More info can be found here:\n",
    "# https://github.com/apache/spark/blob/master/python/pyspark/rdd.py#L1249\n",
    "wc.top(3, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB 5 - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAT_FN = 'SAT_Results.csv'\n",
    "HSD_FN = 'DOE_High_School_Directory_2014-2015.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'DBN'),\n",
       " (1, 'SCHOOL NAME'),\n",
       " (2, 'Num of SAT Test Takers'),\n",
       " (3, 'SAT Critical Reading Avg. Score'),\n",
       " (4, 'SAT Math Avg. Score'),\n",
       " (5, 'SAT Writing Avg. Score')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We read the SAT score to our RDD. Note that the use_unicode can be\n",
    "# changed accordingly to your data file to handle Unicode. If you cannot\n",
    "# parse your data due to an 'utf8' or 'ascii' decoding issue, it might\n",
    "# be a good thing to try flipping the use_unicode parameter here.\n",
    "\n",
    "sat = sc.textFile(SAT_FN, use_unicode=True).cache()\n",
    "\n",
    "# This line for us to list the column index and column names to see\n",
    "# which column we need to use for our task. In this case, we're\n",
    "# interested in the number of test takers (#2) and the math score (#4).\n",
    "list(enumerate(sat.first().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBN,SCHOOL NAME,Num of SAT Test Takers,SAT Critical Reading Avg. Score,SAT Math Avg. Score,SAT Writing Avg. Score\n",
      "----------\n",
      "02M047,47 THE AMERICAN SIGN LANGUAGE AND ENGLISH SECONDARY SCHOOL,16,395,400,387\n"
     ]
    }
   ],
   "source": [
    "# Note that, our data input includes a header line that we don't want to\n",
    "# use in analysis. We can remove the header line from our RDD by doing\n",
    "# a 'filter' to remove all rows that matches the header like below. Though\n",
    "# this works, it means that we have to apply the filter function on *all*\n",
    "# row, which could be a lot of computation.\n",
    "\n",
    "noHeaderRDD = sat.filter(lambda x: not x.startswith('DBN,SCHOOL'))\n",
    "print (sat.first())\n",
    "print(\"----------\")\n",
    "print (noHeaderRDD.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('02M047', (6400, 16)),\n",
       " ('21K410', (207575, 475)),\n",
       " ('30Q301', (43120, 98)),\n",
       " ('17K382', (22066, 59)),\n",
       " ('18K637', (13335, 35)),\n",
       " ('32K403', (18300, 50)),\n",
       " ('09X365', (18306, 54)),\n",
       " ('11X270', (22064, 56)),\n",
       " ('05M367', (12078, 33)),\n",
       " ('14K404', (24276, 68)),\n",
       " ('30Q575', (66420, 135)),\n",
       " ('13K336', (3366, 9)),\n",
       " ('04M635', (17712, 48)),\n",
       " ('24Q264', (40406, 89)),\n",
       " ('17K408', (19494, 57))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, we can perform the header checking per-partition, instead\n",
    "# of per-row like below. mapPartitions() is another type of map operators\n",
    "# in Spark that is similar to Hadoop Streaming's map(). It is many-to-many.\n",
    "# RDD in Spark are divided into partitions (as we read or as provided by\n",
    "# HDFS), each partition can be processed in parallel using a function\n",
    "# supplied to the mapPartitions() call.\n",
    "# \n",
    "# In addition to mapPartitions(), Spark also provides a variation called\n",
    "# mapPartitionsWithIndex() that provides information on which partition\n",
    "# we are currently processing. Indeed, mapPartitionsWithIndex() is the\n",
    "# the operator with the lowest overhead (since mapPartitions() get mapped\n",
    "# to mapPartitionsWithIndex) and also the most efficient one among all the\n",
    "# map operators.\n",
    "#\n",
    "# So our logic below is to use the partition index to check if we're hitting\n",
    "# the header (aka the first partition). If so, we just skip the first row.\n",
    "\n",
    "def extractScores(partId, records):\n",
    "    if partId==0:\n",
    "        next(records)\n",
    "    import csv\n",
    "    reader = csv.reader(records)\n",
    "    for row in reader:\n",
    "        if row[2]!='s': # to filter our bad-quality data\n",
    "            (dbn,takers,score) = (row[0], int(row[2]), int(row[4]))\n",
    "            yield (dbn, (score*takers, takers))\n",
    "\n",
    "satScores = sat.mapPartitionsWithIndex(extractScores)\n",
    "satScores.take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'dbn'),\n",
       " (1, 'school_name'),\n",
       " (2, 'boro'),\n",
       " (3, 'building_code'),\n",
       " (4, 'phone_number'),\n",
       " (5, 'fax_number'),\n",
       " (6, 'grade_span_min'),\n",
       " (7, 'grade_span_max'),\n",
       " (8, 'expgrade_span_min'),\n",
       " (9, 'expgrade_span_max'),\n",
       " (10, 'bus'),\n",
       " (11, 'subway'),\n",
       " (12, 'primary_address_line_1'),\n",
       " (13, 'city'),\n",
       " (14, 'state_code'),\n",
       " (15, 'zip'),\n",
       " (16, 'website'),\n",
       " (17, 'total_students'),\n",
       " (18, 'campus_name'),\n",
       " (19, 'school_type'),\n",
       " (20, 'overview_paragraph'),\n",
       " (21, 'program_highlights'),\n",
       " (22, 'language_classes'),\n",
       " (23, 'advancedplacement_courses'),\n",
       " (24, 'online_ap_courses'),\n",
       " (25, 'online_language_courses'),\n",
       " (26, 'extracurricular_activities'),\n",
       " (27, 'psal_sports_boys'),\n",
       " (28, 'psal_sports_girls'),\n",
       " (29, 'psal_sports_coed'),\n",
       " (30, 'school_sports'),\n",
       " (31, 'partner_cbo'),\n",
       " (32, 'partner_hospital'),\n",
       " (33, 'partner_highered'),\n",
       " (34, 'partner_cultural'),\n",
       " (35, 'partner_nonprofit'),\n",
       " (36, 'partner_corporate'),\n",
       " (37, 'partner_financial'),\n",
       " (38, 'partner_other'),\n",
       " (39, 'addtl_info1'),\n",
       " (40, 'addtl_info2'),\n",
       " (41, 'start_time'),\n",
       " (42, 'end_time'),\n",
       " (43, 'se_services'),\n",
       " (44, 'ell_programs'),\n",
       " (45, 'school_accessibility_description'),\n",
       " (46, 'number_programs'),\n",
       " (47, 'priority01'),\n",
       " (48, 'priority02'),\n",
       " (49, 'priority03'),\n",
       " (50, 'priority04'),\n",
       " (51, 'priority05'),\n",
       " (52, 'priority06'),\n",
       " (53, 'priority07'),\n",
       " (54, 'priority08'),\n",
       " (55, 'priority09'),\n",
       " (56, 'priority10'),\n",
       " (57, 'Location 1')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we do the same thing with the school directory data\n",
    "schools = sc.textFile(HSD_FN, use_unicode=True).cache()\n",
    "list(enumerate(schools.first().split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('01M450', 'Manhattan'),\n",
       " ('01M539', 'Manhattan'),\n",
       " ('01M696', 'Manhattan'),\n",
       " ('02M374', 'Manhattan'),\n",
       " ('02M400', 'Manhattan'),\n",
       " ('02M408', 'Manhattan'),\n",
       " ('02M412', 'Manhattan'),\n",
       " ('02M413', 'Manhattan'),\n",
       " ('02M416', 'Manhattan'),\n",
       " ('02M418', 'Manhattan')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We also process the schools similar to processing the SAT scores, but adding\n",
    "# the logic for keeping only schools with more than 500 students. We practically\n",
    "# apply a filter through mapPartitionsWithIndex\n",
    "\n",
    "def extractSchools(partId, list_of_records):\n",
    "    if partId==0: \n",
    "        next(list_of_records) # skipping the first line\n",
    "    import csv\n",
    "    reader = csv.reader(list_of_records)\n",
    "    for row in reader:\n",
    "        if len(row)==58 and row[17].isdigit():\n",
    "            (dbn, boro, total_students) = (row[0], row[2], int(row[17]))\n",
    "            if total_students>500: # filter to keep the large schools\n",
    "                yield (dbn, boro)\n",
    "\n",
    "largeSchools = schools.mapPartitionsWithIndex(extractSchools)\n",
    "largeSchools.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Manhattan', (28140, 70)),\n",
       " ('Manhattan', (150864, 336)),\n",
       " ('Manhattan', (81663, 167)),\n",
       " ('Manhattan', (38232, 81)),\n",
       " ('Manhattan', (119070, 270))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After that, we perform a join and reduceByKey according to keys (check out the slides)\n",
    "scores = largeSchools.join(satScores).values() \\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "    .mapValues(lambda x: int(x[0]/x[1])) \\\n",
    "    .take(5)\n",
    "largeSchools.join(satScores).values().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'dbn'),\n",
       " (1, 'school_name'),\n",
       " (2, 'boro'),\n",
       " (3, 'building_code'),\n",
       " (4, 'phone_number'),\n",
       " (5, 'fax_number'),\n",
       " (6, 'grade_span_min'),\n",
       " (7, 'grade_span_max'),\n",
       " (8, 'expgrade_span_min'),\n",
       " (9, 'expgrade_span_max'),\n",
       " (10, 'bus'),\n",
       " (11, 'subway'),\n",
       " (12, 'primary_address_line_1'),\n",
       " (13, 'city'),\n",
       " (14, 'state_code'),\n",
       " (15, 'zip'),\n",
       " (16, 'website'),\n",
       " (17, 'total_students'),\n",
       " (18, 'campus_name'),\n",
       " (19, 'school_type'),\n",
       " (20, 'overview_paragraph'),\n",
       " (21, 'program_highlights'),\n",
       " (22, 'language_classes'),\n",
       " (23, 'advancedplacement_courses'),\n",
       " (24, 'online_ap_courses'),\n",
       " (25, 'online_language_courses'),\n",
       " (26, 'extracurricular_activities'),\n",
       " (27, 'psal_sports_boys'),\n",
       " (28, 'psal_sports_girls'),\n",
       " (29, 'psal_sports_coed'),\n",
       " (30, 'school_sports'),\n",
       " (31, 'partner_cbo'),\n",
       " (32, 'partner_hospital'),\n",
       " (33, 'partner_highered'),\n",
       " (34, 'partner_cultural'),\n",
       " (35, 'partner_nonprofit'),\n",
       " (36, 'partner_corporate'),\n",
       " (37, 'partner_financial'),\n",
       " (38, 'partner_other'),\n",
       " (39, 'addtl_info1'),\n",
       " (40, 'addtl_info2'),\n",
       " (41, 'start_time'),\n",
       " (42, 'end_time'),\n",
       " (43, 'se_services'),\n",
       " (44, 'ell_programs'),\n",
       " (45, 'school_accessibility_description'),\n",
       " (46, 'number_programs'),\n",
       " (47, 'priority01'),\n",
       " (48, 'priority02'),\n",
       " (49, 'priority03'),\n",
       " (50, 'priority04'),\n",
       " (51, 'priority05'),\n",
       " (52, 'priority06'),\n",
       " (53, 'priority07'),\n",
       " (54, 'priority08'),\n",
       " (55, 'priority09'),\n",
       " (56, 'priority10'),\n",
       " (57, 'Location 1')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat = sc.textFile(HSD_FN, use_unicode=True).cache()\n",
    "list(enumerate(sat.first().split(',')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
